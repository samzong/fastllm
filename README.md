# FastLLM - è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹æµ‹è¯•æœåŠ¡

ä¸€ä¸ªè½»é‡çº§ã€å•æ–‡ä»¶çš„æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ API æœåŠ¡å™¨ï¼Œä¸“ä¸ºå¿«é€Ÿæµ‹è¯•å’ŒåŸå‹å¼€å‘è€Œè®¾è®¡ã€‚æä¾› OpenAI å…¼å®¹æ¥å£ï¼Œè®©ä½ åœ¨æœ¬åœ°ç¯å¢ƒä¸­å¿«é€Ÿæµ‹è¯•å’Œå¼€å‘ LLM åº”ç”¨ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- ğŸš€ **ä¸€é”®å¯åŠ¨**ï¼šå•æ–‡ä»¶å®ç°ï¼Œé›¶é…ç½®å³å¯å¯åŠ¨æœåŠ¡
- ğŸ” **å¿«é€ŸåŸå‹**ï¼šé€‚åˆå¿«é€ŸéªŒè¯æƒ³æ³•å’Œå¼€å‘åŸå‹
- ğŸ“Š **æµå¼å“åº”**ï¼šæ”¯æŒç±»ä¼¼ ChatGPT çš„å®æ—¶è¾“å‡ºæ•ˆæœ
- ğŸ§© **å³æ’å³ç”¨**ï¼šå®Œå…¨å…¼å®¹ OpenAI APIï¼Œå¯ç›´æ¥æ›¿æ¢ç°æœ‰è°ƒç”¨
- ğŸŒ **å›½å†…å‹å¥½**ï¼šå†…ç½® HuggingFace é•œåƒï¼Œæ— éœ€ç§‘å­¦ä¸Šç½‘
- ğŸ“¥ **ç®€æ˜“éƒ¨ç½²**ï¼šæ”¯æŒæœ¬åœ°æ¨¡å‹å’Œä¸€é”®ä¸‹è½½ HuggingFace æ¨¡å‹

## âš¡ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install vllm fastapi uvicorn torch huggingface_hub
```

### 2. å¯åŠ¨æœåŠ¡

```bash
# ä½¿ç”¨ HuggingFace æ¨¡å‹
python llm_server.py Qwen/Qwen2-0.5B-Instruct

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹
python llm_server.py ./models/my-model
```

### 3. æµ‹è¯•æ¥å£

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2-0.5B-Instruct",
    "messages": [{"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±"}],
    "temperature": 0.7
  }'
```

## ğŸ”§ å‘½ä»¤è¡Œå‚æ•°

```
python llm_server.py [-h] [--gpu-count GPU_COUNT] [--host HOST] [--port PORT] [--no-hf-mirror] [--download-only] model_name
```

| å‚æ•° | è¯´æ˜ |
|------|------|
| `model_name` | æ¨¡å‹åç§°æˆ–è·¯å¾„ (å¿…å¡«) |
| `--gpu-count`, `-g` | ä½¿ç”¨çš„GPUæ•°é‡ |
| `--host` | ç›‘å¬åœ°å€ (é»˜è®¤: 127.0.0.1) |
| `--port`, `-p` | ç›‘å¬ç«¯å£ (é»˜è®¤: 8000) |
| `--no-hf-mirror` | ä¸ä½¿ç”¨HuggingFaceé•œåƒ |
| `--download-only`, `-d` | ä»…ä¸‹è½½æ¨¡å‹ï¼Œä¸å¯åŠ¨æœåŠ¡ |

## ğŸ’¡ å¼€å‘ç¤ºä¾‹

### æ ‡å‡†è°ƒç”¨

```python
import requests

response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "Qwen2-0.5B",
        "messages": [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€è¦ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
        ]
    }
)

print(response.json()["choices"][0]["message"]["content"])
```

### æµå¼è°ƒç”¨

```python
import requests
import json

response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "Qwen2-0.5B",
        "messages": [
            {"role": "user", "content": "å†™ä¸€ç¯‡çŸ­æ–‡ç« ä»‹ç»äººå·¥æ™ºèƒ½"}
        ],
        "stream": True
    },
    stream=True
)

for line in response.iter_lines():
    if line:
        line = line.decode('utf-8')
        if line.startswith('data: '):
            if line == 'data: [DONE]':
                break
            data = json.loads(line[6:])
            delta = data['choices'][0].get('delta', {}).get('content', '')
            if delta:
                print(delta, end='', flush=True)
print()
```

## ğŸ“‹ API å‚è€ƒ

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° |
|------|------|------|
| `/v1/chat/completions` | POST | èŠå¤©å®Œæˆæ¥å£ï¼Œæ”¯æŒæµå¼è¾“å‡º |
| `/v1/models` | GET | è·å–å¯ç”¨æ¨¡å‹ |
| `/health` | GET | å¥åº·æ£€æŸ¥ |

## ğŸš€ é€‚ç”¨åœºæ™¯

- å¿«é€Ÿæµ‹è¯•å„ç§å¼€æº LLM æ¨¡å‹æ•ˆæœ
- åŸå‹é¡¹ç›®å¼€å‘å’ŒéªŒè¯
- æœ¬åœ° AI åº”ç”¨å¼€å‘è°ƒè¯•
- æ•™å­¦å’Œå­¦ä¹  LLM API è°ƒç”¨
- æ— éœ€è”ç½‘çš„ç¦»çº¿æ¼”ç¤º


## ğŸ“ è®¸å¯è¯

MIT License

Copyright (c) 2024 samzong

è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ 