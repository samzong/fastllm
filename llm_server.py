#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import multiprocessing
import argparse
from typing import List, AsyncGenerator
import time
import json
from contextlib import asynccontextmanager

import torch
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from vllm import LLM, SamplingParams
from vllm.outputs import RequestOutput
from pydantic import BaseModel

# é…ç½®å‚æ•°
class Config:
    HOST = "127.0.0.1"       # æœåŠ¡ç›‘å¬åœ°å€
    PORT = 8000           # æœåŠ¡ç›‘å¬ç«¯å£
    USE_HF_MIRROR = True  # æ˜¯å¦ä½¿ç”¨ HuggingFace é•œåƒ
    GPU_COUNT = 1         # ä½¿ç”¨çš„ GPU æ•°é‡
    MODEL_NAME = None      # æ¨¡å‹åç§°ï¼Œå°†é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è®¾ç½®

# å…¨å±€å˜é‡å­˜å‚¨LLMæ¨¡å‹å®ä¾‹
llm_model = None

def get_available_gpu_count() -> int:
    """è·å–å¯ç”¨çš„ GPU æ•°é‡"""
    if torch.cuda.is_available():
        return torch.cuda.device_count()
    return 0

def download_model(model_name: str, use_hf_mirror: bool = True) -> str:
    """åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒæœ¬åœ°è·¯å¾„æˆ–HuggingFaceæ¨¡å‹ """
    # æœ€ç®€å•çš„åˆ¤æ–­ï¼šå¦‚æœè·¯å¾„å­˜åœ¨ï¼Œå°±æ˜¯æœ¬åœ°æ¨¡å‹
    if os.path.exists(model_name):
        print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_name}")
        return model_name
    
    # å¦åˆ™ä»HuggingFaceä¸‹è½½
    from huggingface_hub import snapshot_download
    
    # ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºç›®å½•å
    model_dir = os.path.join(os.getcwd(), model_name.split("/")[-1])
    if not os.path.exists(model_dir):
        os.makedirs(model_dir, exist_ok=True)
    
    try:
        print(f"ğŸ“¥ ä» HuggingFace ä¸‹è½½: {model_name}")
        model_path = snapshot_download(
            repo_id=model_name,
            local_dir=model_dir,
            max_workers=8,
            endpoint="https://hf-mirror.com" if use_hf_mirror else None
        )
        print(f"âœ… ä¸‹è½½å®Œæˆ: {model_path}")
        return model_path
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        raise

def load_model(model_path: str, gpu_count: int = 1) -> LLM:
    """åŠ è½½æ¨¡å‹åˆ°VLLM"""
    available_gpus = get_available_gpu_count()
    if available_gpus > 0:
        print(f"ğŸ–¥ï¸ æ£€æµ‹åˆ°GPU: {available_gpus}ä¸ª")
        gpu_count = min(gpu_count, available_gpus)
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        gpu_count = 1
    
    print("ğŸ”„ åŠ è½½æ¨¡å‹ä¸­...")
    try:
        if torch.cuda.is_available() and gpu_count > 0:
            # é¢„çƒ­æ‰€æœ‰è¦ä½¿ç”¨çš„GPU
            for i in range(gpu_count):
                torch.cuda.set_device(i)
                torch.cuda.empty_cache()
                # åœ¨æ¯ä¸ªGPUä¸Šåˆ›å»ºä¸€ä¸ªå°å¼ é‡æ¥è§¦å‘CUDAåˆå§‹åŒ–
                torch.zeros(1, device=f"cuda:{i}")

        # åŠ è½½LLMæ¨¡å‹
        llm = LLM(
            model=model_path,
            tensor_parallel_size=gpu_count,
            trust_remote_code=True,
            dtype="auto",
            gpu_memory_utilization=0.8,
            quantization=None,  # ç¦ç”¨é‡åŒ–ä»¥é¿å…æ½œåœ¨é—®é¢˜
            max_num_batched_tokens=4096,
            enforce_eager=True,
            max_model_len=8192  # æ˜ç¡®è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
        )
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return llm

@asynccontextmanager
async def lifespan(app: FastAPI):
    """å¤„ç†åº”ç”¨çš„ç”Ÿå‘½å‘¨æœŸäº‹ä»¶"""
    global llm_model
    model_path = download_model(Config.MODEL_NAME, Config.USE_HF_MIRROR)
    llm_model = load_model(model_path, Config.GPU_COUNT)
    print("ğŸš€ æœåŠ¡å°±ç»ª")
    yield
    if llm_model:
        del llm_model
        torch.cuda.empty_cache()
        print("ğŸ›‘ æœåŠ¡å·²åœæ­¢")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="LLM API Service", lifespan=lifespan)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API æ•°æ®æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: float = 0.7
    max_tokens: int = 512
    stream: bool = False

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAIå…¼å®¹çš„èŠå¤©å®Œæˆæ¥å£"""
    if not llm_model:
        raise HTTPException(status_code=503, detail="æ¨¡å‹å°šæœªåŠ è½½å®Œæˆ")
    
    # æ„å»ºæç¤ºæ–‡æœ¬
    prompt = ""
    for msg in request.messages:
        if msg.role == "system":
            prompt += f"System: {msg.content}\n"
        elif msg.role == "user":
            prompt += f"User: {msg.content}\n"
        elif msg.role == "assistant":
            prompt += f"Assistant: {msg.content}\n"
    
    prompt += "Assistant: "
    
    # å¦‚æœè¯·æ±‚æµå¼è¾“å‡º
    if request.stream:
        return StreamingResponse(
            stream_response(prompt, request),
            media_type="text/event-stream"
        )
    
    # éæµå¼è¾“å‡ºå¤„ç†
    outputs = llm_model.generate(
        [prompt],
        SamplingParams(
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
    )
    
    return {
        "id": f"chatcmpl-{int(time.time())}",
        "created": int(time.time()),
        "model": request.model,
        "choices": [{
            "index": 0,
            "message": {"role": "assistant", "content": outputs[0].outputs[0].text},
            "finish_reason": "stop"
        }]
    }

async def stream_response(prompt: str, request: ChatCompletionRequest) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆæµå¼å“åº”"""
    request_id = f"chatcmpl-{int(time.time())}"
    created = int(time.time())
    
    # åˆ›å»ºè¯·æ±‚å‚æ•°
    sampling_params = SamplingParams(
        max_tokens=request.max_tokens,
        temperature=request.temperature
    )
    
    # æäº¤ç”Ÿæˆè¯·æ±‚
    results_generator = llm_model.generate_async(
        [prompt],
        sampling_params,
    )
    
    # æµå¼è¾“å‡ºæ ‡å¤´
    yield "data: " + json.dumps({
        'id': request_id,
        'object': 'chat.completion.chunk',
        'created': created,
        'model': request.model,
        'choices': [{
            'index': 0,
            'delta': {'role': 'assistant'},
            'finish_reason': None
        }]
    }) + "\n\n"
    
    # è¿½è¸ªä¹‹å‰å‘é€çš„å†…å®¹ï¼Œç”¨äºè®¡ç®—å¢é‡
    previous_text = ""
    
    async for result in results_generator:
        output: RequestOutput = result[0]
        if output.outputs:
            current_text = output.outputs[0].text
            # è®¡ç®—å¢é‡æ–‡æœ¬
            delta_text = current_text[len(previous_text):]
            previous_text = current_text
            
            # å‘é€å¢é‡æ›´æ–°
            yield "data: " + json.dumps({
                'id': request_id,
                'object': 'chat.completion.chunk',
                'created': created,
                'model': request.model,
                'choices': [{
                    'index': 0,
                    'delta': {'content': delta_text},
                    'finish_reason': None
                }]
            }) + "\n\n"
    
    # å‘é€ç»“æŸæ ‡è®°
    yield "data: " + json.dumps({
        'id': request_id,
        'object': 'chat.completion.chunk',
        'created': created,
        'model': request.model,
        'choices': [{
            'index': 0,
            'delta': {},
            'finish_reason': 'stop'
        }]
    }) + "\n\n"
    
    # æ ‡å‡†çš„SSEç»“æŸæ ‡è®°
    yield "data: [DONE]\n\n"

@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return {
        "data": [{"id": Config.MODEL_NAME.split("/")[-1]}],
        "object": "list"
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "healthy", "model_loaded": llm_model is not None}

def main():
    """ä¸»å‡½æ•°"""
    # è·å–å¯ç”¨ GPU æ•°é‡å¹¶åˆå§‹åŒ–CUDAç¯å¢ƒ
    available_gpus = get_available_gpu_count()
    default_gpu_count = min(available_gpus, 1) if available_gpus > 0 else 1

    # åˆå§‹åŒ–CUDAç¯å¢ƒ
    if torch.cuda.is_available():
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPU
        if available_gpus > 0:
            # é¢„çƒ­æ‰€æœ‰è¦ä½¿ç”¨çš„GPU
            for i in range(default_gpu_count):
                torch.cuda.set_device(i)
                torch.cuda.empty_cache()
                # åœ¨æ¯ä¸ªGPUä¸Šåˆ›å»ºä¸€ä¸ªå°å¼ é‡æ¥è§¦å‘CUDAåˆå§‹åŒ–
                torch.zeros(1, device=f"cuda:{i}")
            print(f"âœ… CUDAç¯å¢ƒå·²åˆå§‹åŒ– (GPUæ•°é‡: {default_gpu_count})")
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPU")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='OpenAI å…¼å®¹çš„ LLM API æœåŠ¡å™¨',
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    # ä½ç½®å‚æ•°
    parser.add_argument(
        'model_name', 
        type=str,
        help='æ¨¡å‹åç§°æˆ–è·¯å¾„ï¼Œä¾‹å¦‚:\n'
             '- HFæ¨¡å‹: Qwen/Qwen2-0.5B-Instruct\n'
             '- æœ¬åœ°æ¨¡å‹: ./models/my-model\n'
             'æ³¨: å­˜åœ¨è·¯å¾„è§†ä¸ºæœ¬åœ°æ¨¡å‹ï¼Œå¦åˆ™ä»HFä¸‹è½½'
    )
    
    # å¯é€‰å‚æ•°ç»„
    group = parser.add_argument_group('å¯é€‰å‚æ•°')
    
    group.add_argument(
        '--gpu-count', '-g',
        type=int, 
        default=default_gpu_count,
        help=f'ä½¿ç”¨çš„GPUæ•°é‡ (é»˜è®¤: {default_gpu_count})'
    )
    
    group.add_argument(
        '--host', 
        type=str, 
        default=Config.HOST,
        help=f'ç›‘å¬åœ°å€ (é»˜è®¤: {Config.HOST})'
    )
    
    group.add_argument(
        '--port', '-p',
        type=int, 
        default=Config.PORT,
        help=f'ç›‘å¬ç«¯å£ (é»˜è®¤: {Config.PORT})'
    )
    
    group.add_argument(
        '--no-hf-mirror', 
        action='store_true',
        help='ä¸ä½¿ç”¨HFé•œåƒ (é»˜è®¤: ä½¿ç”¨hf-mirror.com)'
    )
    
    group.add_argument(
        '--download-only', '-d',
        action='store_true',
        help='ä»…ä¸‹è½½æ¨¡å‹ï¼Œä¸å¯åŠ¨æœåŠ¡'
    )
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    Config.MODEL_NAME = args.model_name
    Config.GPU_COUNT = args.gpu_count
    Config.HOST = args.host
    Config.PORT = args.port
    Config.USE_HF_MIRROR = not args.no_hf_mirror
    
    # å¤„ç†ä»…ä¸‹è½½æ¨¡å¼
    if args.download_only:
        print(f"ğŸ“¥ ä¸‹è½½æ¨¡å¼ | æ¨¡å‹: {Config.MODEL_NAME}")
        if Config.USE_HF_MIRROR:
            print("   ä½¿ç”¨HFé•œåƒ: hf-mirror.com")
        
        try:
            model_path = download_model(Config.MODEL_NAME, Config.USE_HF_MIRROR)
            print(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {model_path}")
            return
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            return
    
    # å¯åŠ¨æœåŠ¡å™¨æ¨¡å¼
    print(f"ğŸš€ å¯åŠ¨æœåŠ¡ | æ¨¡å‹: {Config.MODEL_NAME}")
    print(f"   åœ°å€: {Config.HOST}:{Config.PORT} | GPU: {Config.GPU_COUNT}")
    if Config.USE_HF_MIRROR:
        print("   ä½¿ç”¨HFé•œåƒ: hf-mirror.com")
    
    # å¯åŠ¨æœåŠ¡ï¼ˆä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼å¹¶é…ç½®uvicornå‚æ•°ï¼‰
    uvicorn.run(
        app,
        host=Config.HOST,
        port=Config.PORT,
        workers=1,
        loop="asyncio",
        http="auto",
        reload=False,
        access_log=False,
        log_level="error",
        factory=False
    )

# åœ¨å¯¼å…¥æ—¶è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
try:
    # è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•ä¸ºspawnï¼Œä»¥é¿å…CUDAåˆå§‹åŒ–é—®é¢˜
    multiprocessing.set_start_method('spawn')
except RuntimeError:
    pass  # å¯èƒ½å·²ç»è®¾ç½®è¿‡äº†ï¼Œå¿½ç•¥é”™è¯¯

if __name__ == "__main__":
    main()
