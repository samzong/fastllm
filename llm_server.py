#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import argparse
from typing import List, AsyncGenerator
import time
import json
from contextlib import asynccontextmanager

import torch
import requests
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from vllm import LLM, SamplingParams
from vllm.outputs import RequestOutput
from pydantic import BaseModel
from huggingface_hub import configure_http_backend

# é…ç½®å‚æ•°
class Config:
    HOST = "127.0.0.1"       # æœåŠ¡ç›‘å¬åœ°å€
    PORT = 8000           # æœåŠ¡ç›‘å¬ç«¯å£
    USE_HF_MIRROR = True  # æ˜¯å¦ä½¿ç”¨ HuggingFace é•œåƒ
    GPU_COUNT = 1         # ä½¿ç”¨çš„ GPU æ•°é‡
    MODEL_NAME = None      # æ¨¡å‹åç§°ï¼Œå°†é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è®¾ç½®

# å…¨å±€å˜é‡å­˜å‚¨LLMæ¨¡å‹å®ä¾‹
llm_model = None

def get_available_gpu_count() -> int:
    """è·å–å¯ç”¨çš„ GPU æ•°é‡"""
    if torch.cuda.is_available():
        return torch.cuda.device_count()
    return 0

def configure_hf_mirror():
    """é…ç½® HuggingFace é•œåƒç«™ç‚¹"""
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå…¼å®¹æ€§æ–¹æ¡ˆï¼‰
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    
    # ä½¿ç”¨å®˜æ–¹æ¨èæ–¹å¼é…ç½®HTTPåç«¯
    def backend_factory() -> requests.Session:
        session = requests.Session()
        session.proxies = {
            "http": "https://hf-mirror.com",
            "https": "https://hf-mirror.com"
        }
        return session
    
    configure_http_backend(backend_factory=backend_factory)
    print("ğŸ“¡ å·²é…ç½® HuggingFace é•œåƒ: hf-mirror.com")

def download_model(model_name: str, use_hf_mirror: bool = True) -> str:
    """åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒæœ¬åœ°è·¯å¾„æˆ–HuggingFaceæ¨¡å‹ """
    # æœ€ç®€å•çš„åˆ¤æ–­ï¼šå¦‚æœè·¯å¾„å­˜åœ¨ï¼Œå°±æ˜¯æœ¬åœ°æ¨¡å‹
    if os.path.exists(model_name):
        print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_name}")
        return model_name
    
    # å¦åˆ™ä»HuggingFaceä¸‹è½½
    from huggingface_hub import snapshot_download
    
    # é…ç½®é•œåƒ
    if use_hf_mirror:
        configure_hf_mirror()
    
    # ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºç›®å½•å
    model_dir = os.path.join(os.getcwd(), model_name.split("/")[-1])
    if not os.path.exists(model_dir):
        os.makedirs(model_dir, exist_ok=True)
    
    try:
        print(f"ğŸ“¥ ä» HuggingFace ä¸‹è½½: {model_name}")
        model_path = snapshot_download(
            repo_id=model_name,
            local_dir=model_dir,
            max_workers=4, 
            local_dir_use_symlinks=False
        )
        print(f"âœ… ä¸‹è½½å®Œæˆ: {model_path}")
        return model_path
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        raise

def load_model(model_path: str, gpu_count: int = 1) -> LLM:
    """åŠ è½½æ¨¡å‹åˆ°VLLM"""
    available_gpus = get_available_gpu_count()
    if available_gpus > 0:
        print(f"ğŸ–¥ï¸ æ£€æµ‹åˆ°GPU: {available_gpus}ä¸ª")
        gpu_count = min(gpu_count, available_gpus)
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        gpu_count = 1
    
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹ä¸­...")
    llm = LLM(
        model=model_path,
        tensor_parallel_size=gpu_count,
        trust_remote_code=True,
        dtype="auto"
    )
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return llm

@asynccontextmanager
async def lifespan(app: FastAPI):
    """å¤„ç†åº”ç”¨çš„ç”Ÿå‘½å‘¨æœŸäº‹ä»¶"""
    global llm_model
    model_path = download_model(Config.MODEL_NAME, Config.USE_HF_MIRROR)
    llm_model = load_model(model_path, Config.GPU_COUNT)
    print(f"ğŸš€ æœåŠ¡å°±ç»ª")
    yield
    if llm_model:
        del llm_model
        torch.cuda.empty_cache()
        print("ğŸ›‘ æœåŠ¡å·²åœæ­¢")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="LLM API Service", lifespan=lifespan)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API æ•°æ®æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: float = 0.7
    max_tokens: int = 512
    stream: bool = False

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAIå…¼å®¹çš„èŠå¤©å®Œæˆæ¥å£"""
    if not llm_model:
        raise HTTPException(status_code=503, detail="æ¨¡å‹å°šæœªåŠ è½½å®Œæˆ")
    
    # æ„å»ºæç¤ºæ–‡æœ¬
    prompt = ""
    for msg in request.messages:
        if msg.role == "system":
            prompt += f"System: {msg.content}\n"
        elif msg.role == "user":
            prompt += f"User: {msg.content}\n"
        elif msg.role == "assistant":
            prompt += f"Assistant: {msg.content}\n"
    
    prompt += "Assistant: "
    
    # å¦‚æœè¯·æ±‚æµå¼è¾“å‡º
    if request.stream:
        return StreamingResponse(
            stream_response(prompt, request),
            media_type="text/event-stream"
        )
    
    # éæµå¼è¾“å‡ºå¤„ç†
    outputs = llm_model.generate(
        [prompt],
        SamplingParams(
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
    )
    
    return {
        "id": f"chatcmpl-{int(time.time())}",
        "created": int(time.time()),
        "model": request.model,
        "choices": [{
            "index": 0,
            "message": {"role": "assistant", "content": outputs[0].outputs[0].text},
            "finish_reason": "stop"
        }]
    }

async def stream_response(prompt: str, request: ChatCompletionRequest) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆæµå¼å“åº”"""
    request_id = f"chatcmpl-{int(time.time())}"
    created = int(time.time())
    
    # åˆ›å»ºè¯·æ±‚å‚æ•°
    sampling_params = SamplingParams(
        max_tokens=request.max_tokens,
        temperature=request.temperature
    )
    
    # æäº¤ç”Ÿæˆè¯·æ±‚
    results_generator = llm_model.generate_async(
        [prompt],
        sampling_params,
    )
    
    # æµå¼è¾“å‡ºæ ‡å¤´
    yield "data: " + json.dumps({
        'id': request_id,
        'object': 'chat.completion.chunk',
        'created': created,
        'model': request.model,
        'choices': [{
            'index': 0,
            'delta': {'role': 'assistant'},
            'finish_reason': None
        }]
    }) + "\n\n"
    
    # è¿½è¸ªä¹‹å‰å‘é€çš„å†…å®¹ï¼Œç”¨äºè®¡ç®—å¢é‡
    previous_text = ""
    
    async for result in results_generator:
        output: RequestOutput = result[0]
        if output.outputs:
            current_text = output.outputs[0].text
            # è®¡ç®—å¢é‡æ–‡æœ¬
            delta_text = current_text[len(previous_text):]
            previous_text = current_text
            
            # å‘é€å¢é‡æ›´æ–°
            yield "data: " + json.dumps({
                'id': request_id,
                'object': 'chat.completion.chunk',
                'created': created,
                'model': request.model,
                'choices': [{
                    'index': 0,
                    'delta': {'content': delta_text},
                    'finish_reason': None
                }]
            }) + "\n\n"
    
    # å‘é€ç»“æŸæ ‡è®°
    yield "data: " + json.dumps({
        'id': request_id,
        'object': 'chat.completion.chunk',
        'created': created,
        'model': request.model,
        'choices': [{
            'index': 0,
            'delta': {},
            'finish_reason': 'stop'
        }]
    }) + "\n\n"
    
    # æ ‡å‡†çš„SSEç»“æŸæ ‡è®°
    yield "data: [DONE]\n\n"

@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return {
        "data": [{"id": Config.MODEL_NAME.split("/")[-1]}],
        "object": "list"
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "healthy", "model_loaded": llm_model is not None}

def main():
    """ä¸»å‡½æ•°"""
    # è·å–å¯ç”¨ GPU æ•°é‡
    available_gpus = get_available_gpu_count()
    default_gpu_count = min(available_gpus, 1) if available_gpus > 0 else 1
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='OpenAI å…¼å®¹çš„ LLM API æœåŠ¡å™¨',
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    # ä½ç½®å‚æ•°
    parser.add_argument(
        'model_name', 
        type=str,
        help='æ¨¡å‹åç§°æˆ–è·¯å¾„ï¼Œä¾‹å¦‚:\n'
             '- HFæ¨¡å‹: Qwen/Qwen2-0.5B-Instruct\n'
             '- æœ¬åœ°æ¨¡å‹: ./models/my-model\n'
             'æ³¨: å­˜åœ¨è·¯å¾„è§†ä¸ºæœ¬åœ°æ¨¡å‹ï¼Œå¦åˆ™ä»HFä¸‹è½½'
    )
    
    # å¯é€‰å‚æ•°ç»„
    group = parser.add_argument_group('å¯é€‰å‚æ•°')
    
    group.add_argument(
        '--gpu-count', '-g',
        type=int, 
        default=default_gpu_count,
        help=f'ä½¿ç”¨çš„GPUæ•°é‡ (é»˜è®¤: {default_gpu_count})'
    )
    
    group.add_argument(
        '--host', 
        type=str, 
        default=Config.HOST,
        help=f'ç›‘å¬åœ°å€ (é»˜è®¤: {Config.HOST})'
    )
    
    group.add_argument(
        '--port', '-p',
        type=int, 
        default=Config.PORT,
        help=f'ç›‘å¬ç«¯å£ (é»˜è®¤: {Config.PORT})'
    )
    
    group.add_argument(
        '--no-hf-mirror', 
        action='store_true',
        help='ä¸ä½¿ç”¨HFé•œåƒ (é»˜è®¤: ä½¿ç”¨hf-mirror.com)'
    )
    
    group.add_argument(
        '--download-only', '-d',
        action='store_true',
        help='ä»…ä¸‹è½½æ¨¡å‹ï¼Œä¸å¯åŠ¨æœåŠ¡'
    )
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    Config.MODEL_NAME = args.model_name
    Config.GPU_COUNT = args.gpu_count
    Config.HOST = args.host
    Config.PORT = args.port
    Config.USE_HF_MIRROR = not args.no_hf_mirror
    
    # å¤„ç†ä»…ä¸‹è½½æ¨¡å¼
    if args.download_only:
        print(f"ğŸ“¥ ä¸‹è½½æ¨¡å¼ | æ¨¡å‹: {Config.MODEL_NAME}")
        if Config.USE_HF_MIRROR:
            print("   ä½¿ç”¨HFé•œåƒ: hf-mirror.com")
        
        try:
            model_path = download_model(Config.MODEL_NAME, Config.USE_HF_MIRROR)
            print(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {model_path}")
            return
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            return
    
    # å¯åŠ¨æœåŠ¡å™¨æ¨¡å¼
    print(f"ğŸš€ å¯åŠ¨æœåŠ¡ | æ¨¡å‹: {Config.MODEL_NAME}")
    print(f"   åœ°å€: {Config.HOST}:{Config.PORT} | GPU: {Config.GPU_COUNT}")
    if Config.USE_HF_MIRROR:
        print("   ä½¿ç”¨HFé•œåƒ: hf-mirror.com")
    
    # å¯åŠ¨æœåŠ¡
    uvicorn.run(app, host=Config.HOST, port=Config.PORT)

if __name__ == "__main__":
    main() 